{
  "id": "fc_68b32723aff081959bf4818b0b2f7c3a0ab6361ec9f89aa1",
  "type": "FunctionCall",
  "name": "shell",
  "args": "{\"command\":[\"apply_patch\",\"*** Begin Patch\\n*** Add File: src/split.py\\n+from __future__ import annotations\\n+\\n+import hashlib\\n+import math\\n+import random\\n+from dataclasses import dataclass\\n+from typing import Callable, Dict, Iterable, List, Literal, Sequence, Tuple\\n+\\n+from src.models import DataRecord\\n+\\n+\\n+StratifyBy = Literal[\\\"none\\\", \\\"source\\\", \\\"primary_tag\\\"]\\n+\\n+\\n+def _stable_int(s: str) -> int:\\n+    return int(hashlib.sha256(s.encode(\\\"utf-8\\\")).hexdigest(), 16)\\n+\\n+\\n+def _group_key(rec: DataRecord, mode: StratifyBy) -> str:\\n+    if mode == \\\"none\\\":\\n+        return \\\"__all__\\\"\\n+    if mode == \\\"source\\\":\\n+        return rec.meta.source\\n+    if mode == \\\"primary_tag\\\":\\n+        return rec.meta.tags[0] if rec.meta.tags else \\\"__no_tag__\\\"\\n+    raise ValueError(f\\\"unknown stratify mode: {mode}\\\")\\n+\\n+\\n+@dataclass\\n+class SplitResult:\\n+    train: List[DataRecord]\\n+    val: List[DataRecord]\\n+    test: List[DataRecord]\\n+\\n+\\n+def split_records(\\n+    records: Sequence[DataRecord],\\n+    *,\\n+    train_ratio: float = 0.8,\\n+    val_ratio: float = 0.1,\\n+    test_ratio: float = 0.1,\\n+    seed: int = 42,\\n+    stratify_by: StratifyBy = \\\"source\\\",\\n+) -> SplitResult:\\n+    \\\"\\\"\\\"Deterministic stratified split of DataRecord sequence.\\n+\\n+    - Ratios must sum to 1.0\\n+    - Stratification by `source` (default), `primary_tag`, or \\\"none\\\".\\n+    - Deterministic via fixed seed and stable sha256-derived per-group seeds.\\n+    \\\"\\\"\\\"\\n+    n = len(records)\\n+    if n == 0:\\n+        return SplitResult([], [], [])\\n+    total = train_ratio + val_ratio + test_ratio\\n+    if not math.isclose(total, 1.0, rel_tol=0, abs_tol=1e-9):\\n+        raise ValueError(\\\"train/val/test ratios must sum to 1.0\\\")\\n+\\n+    # Target global counts\\n+    target_train = math.floor(n * train_ratio)\\n+    target_val = math.floor(n * val_ratio)\\n+    target_test = n - target_train - target_val\\n+\\n+    # Group records by stratify key\\n+    groups: Dict[str, List[DataRecord]] = {}\\n+    for rec in records:\\n+        key = _group_key(rec, stratify_by)\\n+        groups.setdefault(key, []).append(rec)\\n+\\n+    # Compute per-group base counts and fractional remainders for each split\\n+    def per_split_counts(group_size: int, ratio: float) -> Tuple[int, float]:\\n+        exact = group_size * ratio\\n+        base = math.floor(exact)\\n+        frac = exact - base\\n+        return base, frac\\n+\\n+    # Initialize allocations\\n+    alloc_train: Dict[str, int] = {}\\n+    alloc_val: Dict[str, int] = {}\\n+    alloc_test: Dict[str, int] = {}\\n+\\n+    base_train = base_val = base_test = 0\\n+    fracs_train: List[Tuple[str, float]] = []\\n+    fracs_val: List[Tuple[str, float]] = []\\n+    fracs_test: List[Tuple[str, float]] = []\\n+\\n+    for key, items in groups.items():\\n+        m = len(items)\\n+        b, f = per_split_counts(m, train_ratio)\\n+        alloc_train[key] = b\\n+        base_train += b\\n+        fracs_train.append((key, f))\\n+\\n+        b, f = per_split_counts(m, val_ratio)\\n+        alloc_val[key] = b\\n+        base_val += b\\n+        fracs_val.append((key, f))\\n+\\n+        # test remainder will be decided after train+val; use ratio-based too\\n+        b, f = per_split_counts(m, test_ratio)\\n+        alloc_test[key] = b\\n+        base_test += b\\n+        fracs_test.append((key, f))\\n+\\n+    # Distribute remaining counts by largest fractional remainder with deterministic tie-break\\n+    def distribute(\\n+        target_total: int,\\n+        base_total: int,\\n+        alloc: Dict[str, int],\\n+        fracs: List[Tuple[str, float]],\\n+    ) -> None:\\n+        remaining = target_total - base_total\\n+        if remaining <= 0:\\n+            return\\n+        fracs_sorted = sorted(\\n+            fracs,\\n+            key=lambda kv: (kv[1], _stable_int(kv[0])),\\n+            reverse=True,\\n+        )\\n+        for i in range(remaining):\\n+            k = fracs_sorted[i % len(fracs_sorted)][0]\\n+            alloc[k] += 1\\n+\\n+    distribute(target_train, base_train, alloc_train, fracs_train)\\n+    distribute(target_val, base_val, alloc_val, fracs_val)\\n+    distribute(target_test, base_test, alloc_test, fracs_test)\\n+\\n+    # Now, for each group, shuffle deterministically and slice\\n+    train: List[DataRecord] = []\\n+    val: List[DataRecord] = []\\n+    test: List[DataRecord] = []\\n+\\n+    for key, items in groups.items():\\n+        local = list(items)\\n+        rng = random.Random(seed ^ (_stable_int(key) & ((1 << 63) - 1)))\\n+        rng.shuffle(local)\\n+        t = alloc_train[key]\\n+        v = alloc_val[key]\\n+        w = alloc_test[key]\\n+        # Adjust in case rounding overflowed group size due to numeric issues\\n+        if t + v + w != len(local):\\n+            # Recompute test as the remainder\\n+            w = max(0, len(local) - t - v)\\n+        train.extend(local[:t])\\n+        val.extend(local[t : t + v])\\n+        test.extend(local[t + v : t + v + w])\\n+\\n+    # Sanity checks: no duplicates across splits\\n+    def _ids(seq: Iterable[DataRecord]) -> set[str]:\\n+        return {r.id for r in seq}\\n+\\n+    ids_train, ids_val, ids_test = _ids(train), _ids(val), _ids(test)\\n+    if ids_train & ids_val or ids_train & ids_test or ids_val & ids_test:\\n+        raise AssertionError(\\\"duplicate ids detected across splits\\\")\\n+\\n+    return SplitResult(train=train, val=val, test=test)\\n+\\n*** End Patch\"]}"
}
